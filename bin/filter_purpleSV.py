#!/usr/bin/env python3

# From a VCF of SVs generated by manta, filter out SVs:
# - whose length is too short
# - which have insufficient Read Pair or Split Reads support
# - (optionally) for which one breakend is in a region present in the Panel of Normal
# - short insertions which likely correspond to retrotransposons (except when they are in the vicinity of a gene). This filtering requires 2 breakpoints.
# (optionally) flags SVs for which one breakend is close to a low mappability region
# (optionally) checks that deletions/duplications are supported by the coverage

import os
import sys
import argparse
import gzip
import numpy as np
import pandas as pd
import vcfpy
import pysam

parser = argparse.ArgumentParser()
parser.add_argument('-i', type = str, help='VCF generated by manta')
parser.add_argument('-o', type = str, help='Output VCF')
parser.add_argument('--log', type = str, help='File where the log will be stored')
parser.add_argument('--pon', type = str, help='bed file containing regions to filter out.')
parser.add_argument('--minPR', type = int,default=0, help='Minimum number of paired reads supporting the SV')
parser.add_argument('--minSR', type = int,default=0, help='Minimum number of split reads supporting the SV')
parser.add_argument('--minLen', type = int,default=20000, help='Minimum SV length')
parser.add_argument('--bam', type = str, help='BAM file (used to check the breakpoints')
parser.add_argument('--mappability', type = str, help='bed file containing the regions with low mappability')
parser.add_argument('--cnv', type = str, help='cnv file produced by control freec. Less stringent filters are used for SVs close to CNA borders.')
parser.add_argument('--tumorindex', type = int,default=0, help='Index of the tumor sample (0 if only the tumor sample was provided, 1 if a control was used).')
parser.add_argument('--keepCloseSV', type = int,default=0, help='If set to 1, will not filter out two SVs whose breakpoints are close to each other (could be reciprocal translocation)')
parser.add_argument('--filterSmallInsertions', type = int,default=1, help='If set to 0, will not filter out SVs resulting in small insertions.')
parser.add_argument('--puretumor', type = int,default=0, help='If set to 1, will assume that the tumor is pure and will not filter out when no reads support the reference allele..')
args = parser.parse_args()


#sample = "C010-AML-15PB8708"
#args.i = "/home/e840r/Documents/WGS/SVs/manta/filtered/"+sample+".vcf"
#args.o = "/home/e840r/Documents/WGS/SVs/manta/filtered_PoN/"+sample+".vcf"
#args.f= "/home/e840r/Documents/WGS/healthy/SV/PoN_SV.bed"

window_size = 3000 # for filtering out small insertions. 

chromosomes = ["chr"+str(x) for x in range(1,23)] + [str(x) for x in range(1,23)] +["X","Y","chrX","chrY"]
chromosomes2 = ["chr"+str(x) for x in range(1,23)] + [str(x) for x in range(1,23)] +["X","Y","chrX","chrY","hs37d5","MT"]

if args.log is not None:
    log_file = open(args.log,"w")
    sys.stdout = log_file

def locus_in_bedRegion(file,chr,pos):
    """
    Parse the bed files of filtered regions to see if one position is present in one of these regions.
    Start from the last position, to avoid iterating several times through the file (the files are assumed to be sorted)
    """
    current_chr = "0"
    start = 0
    end=0
    line = True
    #print("------ Looking for "+chr+"_"+str(pos))
  
    last_pos_file = file.tell()
    pos_file = file.tell()
    while line and current_chr!=chr: # get to the right chromosome
        #print(line)
        last_pos_file = file.tell()
        pos_file = file.tell()
        line = file.readline()
        if line:
            linesplit = line.split("\t")
            current_chr = linesplit[0]
            start = int(linesplit[1])
            end = int(linesplit[2])
  
    while line and current_chr==chr and end<pos:
        last_pos_file = pos_file
        pos_file = file.tell()
        line = file.readline()
        if line:
            linesplit = line.split("\t")
            current_chr = linesplit[0]
            start = int(linesplit[1])
            end = int(linesplit[2])
    file.seek(last_pos_file) # go back one line 

    if pos <= end and pos >= start:
        return True
    else:
        return False

def bp_in_pon(file,chr1,pos1,chr2,pos2,margin=5):
    """
    Parse the bed files of filtered regions to see if one position is present in one of these regions.
    Start from the last position, to avoid iterating several times through the file (the files are assumed to be sorted)
    """
    current_chr = "0"
    start = 0
    end=0
    line = True
    #print("------ Looking for "+chr+"_"+str(pos))
  
    last_pos_file = file.tell()
    pos_file = file.tell()
    while line and current_chr!=chr1: # get to the right chromosome
        #print(line)
        last_pos_file = file.tell()
        pos_file = file.tell()
        line = file.readline()
        if line:
            line = line.decode("ascii")
            linesplit = line.split("\t")
            current_chr = linesplit[0]
            start = int(linesplit[1])
            end = int(linesplit[2])
  
    while line and current_chr==chr1 and end+margin<pos1:
        last_pos_file = pos_file
        pos_file = file.tell()
        line = file.readline()
        if line:
            line = line.decode("ascii")
            linesplit = line.split("\t")
            current_chr = linesplit[0]
            start = int(linesplit[1])
            end = int(linesplit[2])
            current_chr2 =  linesplit[3]
            start2 =  int(linesplit[4])
            end2 =  int(linesplit[5])
    file.seek(last_pos_file) # go back one line 

    if pos1 <= end+margin and pos1+margin >= start and current_chr2 ==chr2 and pos2 <= end2+margin and pos2+margin >= start2:
        return True
    else:
        return False




# Get all of the positions of the breakends
breakpoints=set()
positions = set()
reader = vcfpy.Reader.from_path(args.i)
for record in reader:
    chr = str(record.CHROM)
    pos = record.POS
    if not chr in chromosomes2: continue
    positions.add((chr,pos))
    if record.INFO["SVTYPE"][:3] in ["DEL","DUP","INS","INV"]:
        chr2 = str(chr)
        pos2 = record.INFO["END"]
    else:
        chr2 = str(record.ALT[0].mate_chrom)
        pos2 = record.ALT[0].mate_pos
    if not chr2 in chromosomes2: continue
    positions.add((chr2,pos2))
    breakpoints.add((chr,pos,chr2,pos2))

def sort_chr_pos(t):
    chr = t[0]
    pos = t[1]
    if chr.isdigit(): chr = int(chr)
    else:
        if chr=="X": chr = 23
        else: chr=24
    return chr * 400000000 + pos

positions = sorted(list(positions), key = sort_chr_pos)
breakpoints = sorted(list(breakpoints), key = sort_chr_pos)
def round_position(x):
    return 100*(x//100)

def bps_close(bp1,bp2,margin=200):
    return bp1[0]==bp2[0] and bp1[2]==bp2[2] and abs(bp1[1]-bp2[1])<=margin and abs(bp1[3]-bp2[3]) <=margin


breakpoints_chr_grouped={}
for (chr,pos,chr2,pos2) in breakpoints:
    pos_rounded = round_position(pos)
    if (not chr in breakpoints_chr_grouped):
        breakpoints_chr_grouped[chr] = {}
    if (not pos_rounded in breakpoints_chr_grouped[chr]):
        breakpoints_chr_grouped[chr][pos_rounded] = []
    breakpoints_chr_grouped[chr][pos_rounded].append((chr,pos,chr2,pos2))

# Go through the PoN file to see which of the breakends are filtered out
pos_is_filtered={}
if args.pon is not None:
    print("PoN was provided")
    file_filter = open(args.pon,"r")
    for pos in positions:
        pos_is_filtered[pos] = locus_in_bedRegion(file_filter,pos[0],pos[1])
    file_filter.close()
else:
    print("PoN was not provided")
    for pos in positions:
        pos_is_filtered[pos] = False


#bp_is_filtered={}
#if args.pon is not None:
#    print("PoN was provided")
#    if args.pon.endswith("gz"):
#        file_filter = gzip.open(args.pon,"r")
#    else:
#        file_filter = open(args.pon,"r")
#    for bp in breakpoints:
#        bp_is_filtered[bp]= bp_in_pon(file_filter,bp[0],bp[1],bp[2],bp[3])
#    file_filter.close()
#else:
#    print("PoN was not provided")
#    for bp in breakpoints:
#        bp_is_filtered[bp] = False


def collect_SVs_close(chr,pos,w):
    records= []
    reader_unfiltered = vcfpy.Reader.from_path(args.i)
    for record in reader_unfiltered:
        if chr == record.CHROM and abs(pos-record.POS) < w and pos!=record.POS:
            records.append(record)
    return records

#############################################
# COVERAGE 
def compute_coverage_region(samfile,chr,start,end,binsize=100000):
    # Divide region into bins to avoid a very high memory usage
    subset_bins = ( end - start > 40*binsize ) # if region is too large, randomly select a subset of bins to estimate the coverage. 
    bins = [start]
    current_pos = start + binsize
    while current_pos < end:
        bins.append(current_pos)
        current_pos+=binsize
    bins.append(end)
    
    bins_index = range(0,len(bins)-2) if not subset_bins else np.random.choice(range(0,len(bins)-2),20,replace=False)
    means = []
    for i in bins_index:
        try:
            c = samfile.count_coverage(contig=chr,start=bins[i],stop=bins[i]+binsize)
            c = np.sum([np.mean(x) for x in c])
            if c>2:
                means.append(c)
        except:
            pass
    sum_coverage = np.sum(means)

    # last bin might have a different size
    if not subset_bins:
        try:
            c = samfile.count_coverage(contig=chr,start=bins[-2],stop=bins[-1])
            c = np.sum([np.mean(x) for x in c])
            if c>2:
                sum_coverage+= (bins[-1]-bins[-2]) / binsize * c
        except:
            pass
    if subset_bins:
        avg_coverage = sum_coverage / len(bins_index)
    else:
        avg_coverage = sum_coverage / ( len(bins_index) + (bins[-1]-bins[-2]) / binsize )
    return avg_coverage

def estimate_global_coverage(samfile):
    """ Selects some random regions instead of actually doing it on the whole genome. Avoid regions with 0 coverage."""
    means = []
    for chr in range(1,23):
        chr = str(chr)
        for pos in np.random.randint(1,2*10**8,5):
            try:
                cov = compute_coverage_region(samfile,chr,pos,pos+50000)
                if cov>2:
                    means.append(cov)
            except:
                pass
    return np.mean(means)

global_coverage=-1
if args.bam is not None:
    samfile = pysam.AlignmentFile(args.bam, "rb")
    global_coverage = estimate_global_coverage(samfile)
    print("Global coverage: " + str(global_coverage))
else:
    print("BAM file was not provided")
# END COVERAGE
####################################################


def get_breakpoint_info(record):
    chr = record.CHROM
    pos = record.POS
    if record.INFO["SVTYPE"] in ["DEL","DUP","INS"]:
        chr2 = chr
        pos2 = record.INFO["END"]
        if record.INFO["SVTYPE"]=="DEL":
            orientation = "-"
            orientation2 = "+"
        elif record.INFO["SVTYPE"]=="DUP":
            orientation = "+"
            orientation2 = "-"
        else:
            orientation = "-"
            orientation2 = "+"
    else:
        chr2 = record.ALT[0].mate_chrom
        pos2 = record.ALT[0].mate_pos
        orientation = record.ALT[0].orientation
        orientation2 = record.ALT[0].mate_orientation
    return ( (chr,pos,orientation) , (chr2,pos2,orientation2) )


def low_mapq_in_region(samfile,chr,start,end,mapq_threshold=45):
    count_lowmapq=0
    count_highmapq=0
    for read in samfile.fetch(chr,start,end):
        if read.mapq<mapq_threshold:
            count_lowmapq+=1
        else:
            count_highmapq+=1
    print("low mapq in region " + str(count_lowmapq)+"; high "+str(count_highmapq))
    if pos_close_to_CNA(chr,(start+end)/2):
        return count_lowmapq/(count_lowmapq+count_highmapq) >=0.40
    else:
        return count_lowmapq/(count_lowmapq+count_highmapq) >=0.2

def low_mapq_SV(samfile,chr,pos,chr2,pos2,min_reads,mapq_threshold=45):
    count_supporting_highMAPQ=0
    count_supporting_lowMAPQ=0
    for read in samfile.fetch(chr,pos-200,pos+200):
        if read.next_reference_name==chr2 and abs(read.next_reference_start-pos2)<1000:
            if read.mapq<mapq_threshold:
                count_supporting_lowMAPQ+=1
            else:
                count_supporting_highMAPQ+=1
    print("Read supporting the SV: low mapq " + str(count_supporting_lowMAPQ)+"; high mapq " + str(count_supporting_highMAPQ))
    return count_supporting_highMAPQ<min_reads or count_supporting_highMAPQ*2 < count_supporting_lowMAPQ

def SV_explained_by_alternative_alignments(samfile,chr1,pos1,chr2,pos2):
    # Find reads supporting the alignment
    count_reads_supportingSV=0
    count_reads_supportingSV_withXA=0
    count_reads_supportingSV_withXA_correct=0
    
    for read in samfile.fetch(chr1, pos1-200, pos1+200):
        # Check for split-read (supplementary alignment)
        SA_chr=""
        for t in read.tags:
            if t[0]=="SA":
                for s in t[1][:-1].split(";"):
                    s_split = s.split(",")
                    s_chr = s_split[0]
                    s_start = int(s_split[1])
                    if (s_chr==chr2 and abs(pos2-s_start)<600):
                        SA_chr=s_chr
                        SA_start = s_start
        # Check for read pair supporting the SV
        read_pair_SV = read.next_reference_name==chr2 and abs(read.next_reference_start-pos2)<1000

        if SA_chr!="" or read_pair_SV: # The read supports the SV
            count_reads_supportingSV+=1
            if read.has_tag("XA"):
                count_reads_supportingSV_withXA+=1
                tag_XA = read.get_tag("XA")
                chr_XA = tag_XA.split(",")[0]
                pos_XA = int(tag_XA.split(",")[1][1:])
                if chr_XA == chr2 and abs (pos_XA-pos2)<=100000:
                    count_reads_supportingSV_withXA_correct+=1
                        

    print("Reads supporting SV: " +str(count_reads_supportingSV)+"; with XA: "+str(count_reads_supportingSV_withXA)+ "; with correct XA: "+str(count_reads_supportingSV_withXA_correct))
    return (count_reads_supportingSV_withXA_correct>0.5*count_reads_supportingSV or (count_reads_supportingSV-count_reads_supportingSV_withXA)<args.minPR)


def filter_min_SR_PR(samfile,chr1,pos1,chr2,pos2):
    # Check that we have enough split reads and paired reads supporting the SV, ignoring those with low mapqual or with alternative alignments
    reads_SR=set()
    reads_PR=set()
    reads_filtered=set()
    mapq1_PR={}
    for read in samfile.fetch(chr1, pos1-200, pos1+200):
        if read.is_supplementary: continue
        has_XA=False
        has_SA=False
        for t in read.tags:
                if t[0]=="XA": has_XA=True
                elif t[0]=="SA": has_SA=True
        if (has_XA and not has_SA) or (read.mapq<=35 and not has_SA) or read.mapq<=10:
            reads_filtered.add(read.query_name)
            continue
        # Check for split-read (supplementary alignment)
        has_correct_SA=False
        for t in read.tags:
            if t[0]=="SA":
                for s in t[1][:-1].split(";"):
                    s_split = s.split(",")
                    s_chr = s_split[0]
                    s_start = int(s_split[1])
                    if (s_chr==chr2 and abs(pos2-s_start)<1000):
                        reads_SR.add(read.query_name)
                        has_correct_SA=True
        # Check for read pair supporting the SV
        if read.next_reference_name==chr2 and abs(read.next_reference_start-pos2)<1000 and ((not has_XA) or has_correct_SA):
            reads_PR.add(read.query_name)
            mapq1_PR[read.query_name]=read.mapq

    mapq2_PR={}
    for read in samfile.fetch(chr2, max(1,pos2-200), pos2+200):
        if read.is_supplementary: continue
        has_XA=False
        has_SA=False
        for t in read.tags:
                if t[0]=="XA": has_XA=True
                elif t[0]=="SA": has_SA=True
        if (has_XA and not has_SA) or (read.mapq<=35 and not has_SA) or read.mapq<=10:
            reads_filtered.add(read.query_name)
            continue
        # Check for split-read (supplementary alignment)
        has_correct_SA=False
        for t in read.tags:
            if t[0]=="SA":
                for s in t[1][:-1].split(";"):
                    s_split = s.split(",")
                    s_chr = s_split[0]
                    s_start = int(s_split[1])
                    if (s_chr==chr and abs(pos-s_start)<1000):
                        reads_SR.add(read.query_name)
                        has_correct_SA=True
        # Check for read pair supporting the SV
        if read.next_reference_name==chr and abs(read.next_reference_start-pos)<1000 and ((not has_XA) or has_correct_SA):
            reads_PR.add(read.query_name)
            mapq2_PR[read.query_name]=read.mapq

    for x in reads_filtered:
        if x in reads_PR:
            reads_PR.remove(x)
        if x in reads_SR:
            reads_SR.remove(x)
    
    # Make sure that, on each side, at least one read supporting the SV has a high mapq.
    max_mapq1=0
    max_mapq2=0
    for x in mapq1_PR:
        if not x in reads_filtered: max_mapq1 = max(max_mapq1,mapq1_PR[x])
    for x in mapq2_PR:
        if not x in reads_filtered: max_mapq2 = max(max_mapq2,mapq2_PR[x])
        

    print("PR " +str(len(reads_PR))+"; SR "+str(len(reads_SR)))
    print(reads_PR)
    print(reads_SR)
    print("Max mapq: "+str(max_mapq1)+","+str(max_mapq2))
    
    return len(reads_PR)<args.minPR or len(reads_SR)<args.minSR or max_mapq1<50 or max_mapq2<50

def many_mates_in_region(samfile,chr,pos,stringent):
    # In some regions, there are reads whose mates map to many different regions. These regions are unreliable.
    regions_mates={}
    for read in samfile.fetch(chr, pos-20, pos+20):
        if read.mapq>=45:
            new_region=True
            for (chrom,position) in regions_mates:
                if chrom==read.next_reference_name and abs(position-read.next_reference_start)<50000:
                    new_region=False
                    regions_mates[(chrom,position)]+=1
            if new_region:
                regions_mates[(read.next_reference_name,read.next_reference_start)] =1
    print("Regions of mates: ")
    print(regions_mates)
    count_regions=0
    for x in regions_mates:
        if regions_mates[x]>2: count_regions+=1
    if not stringent: # less stringent filters is a CNA is close to the SV.
        return count_regions>=7 or len(regions_mates)>=20
    else:
        return count_regions>=5 or len(regions_mates)>=12

def reads_go_through_insertion(samfile,chr1,pos1,chr2,pos2):
    """Look for read pairs which go through an insertion"""
    count_insertions=0
    for read in samfile.fetch(chr1,pos1-400,pos1+400):
        if readpair_goes_through_insertion(read,chr1,pos1,chr2,pos2,1000):
            count_insertions+=1
    for read in samfile.fetch(chr2,pos2-400,pos2+400):
        if readpair_goes_through_insertion(read,chr2,pos2,chr1,pos1,1000):
            count_insertions+=1
    if count_insertions>0:
        print("Insertion count: "+ str(count_insertions))
    return count_insertions>0


def readpair_goes_through_insertion(read,chr1,pos1,chr2,pos2,window_size=1000):
    """Look for a read pair where both main alignments are in the same region, but there is a supplementary alignment in between which maps to the other region."""
    has_SA=False
    for t in read.tags:
        if t[0]=="SA":
            for s in t[1][:-1].split(";"):
                s_split = s.split(",")
                s_chr = s_split[0]
                s_start = int(s_split[1])
                if (s_chr==chr1 and abs(pos1-s_start)<window_size) or (s_chr==chr2 and abs(s_start-pos2) < window_size):
                    SA_chr=s_chr
                    SA_start = s_start
                    has_SA=True
    if not has_SA: return False

    # Check that both read pairs map to the same region
    if read.next_reference_name != read.reference_name or abs(read.next_reference_start-read.reference_start)>window_size: return False

    # Check that the supplementary alignment is between the main alignment and the mate
    if read.cigarstring.find("S")>0: charS="S"
    if read.cigarstring.find("H")>0: charS="H"
    if read.is_reverse:
        sup_alignment_middle = read.cigarstring.find(charS) < read.cigarstring.find("M")
    else:
        sup_alignment_middle = read.cigarstring.find(charS) > read.cigarstring.find("M")
    if not sup_alignment_middle: return False

    # Check that the main alignment maps to one of the 2 regions and that the supplementary alignment maps to the other region
    if read.reference_name==chr1 and abs(read.reference_start-pos1)<window_size:
        if SA_chr==chr2 and abs(SA_start-pos2)<window_size:
            return True
    elif read.reference_name==chr2 and abs(read.reference_start-pos2)<window_size:
        if SA_chr==chr1 and abs(SA_start-pos1)<window_size:
            return True

    return False

def reads_bordered_by_two_breakpoints(samfile,chr,pos1,pos2):
    """Look for reads which have soft or hard clipping on both ends, and which start and end at the correct locations."""
    pos1,pos2 = min(pos1,pos2),max(pos1,pos2)
    count_insertions=0
    for read in samfile.fetch(chr,pos1-100,pos2+100):
        if abs(read.reference_start-pos1)<=6 and abs(read.query_alignment_length+read.reference_start-pos2)<=6:
            if (read.cigarstring.endswith("S") or read.cigarstring.endswith("H")):
                if ((read.cigarstring.find("S")>0 and read.cigarstring.find("S")<read.cigarstring.find("M")) or (read.cigarstring.find("H")>0 and read.cigarstring.find("H")<read.cigarstring.find("M")) ):
                    count_insertions+=1
    if count_insertions>0:
        print("Insertion count: "+ str(count_insertions))
    return count_insertions>=4
#########################################################

# Read the CNV file
CNA_breakpoints={}
if args.cnv is not None:
    df_CN = pd.read_csv(args.cnv,sep="\t")
    for x in df_CN.index:
        chr = df_CN.loc[x,"chromosome"]
        if abs(df_CN.loc[x,"copyNumber"]-2)>0.3 and df_CN.loc[x,"end"] - df_CN.loc[x,"start"]>50000:
            if not chr in CNA_breakpoints:
                CNA_breakpoints[chr]=[]
            CNA_breakpoints[chr].append(df_CN.loc[x,"start"])
            CNA_breakpoints[chr].append(df_CN.loc[x,"end"] )

def pos_close_to_CNA(chr,pos):
    if chr in CNA_breakpoints:
        for pos2 in CNA_breakpoints[chr]:
            if abs(pos-pos2)<=30000:
                return True
    return False


def write_record(writer,record):
    if record.INFO["SVTYPE"]=="BND":
        writer.write_record(record)
    else:
        print("RRRRRRRRRRRRRRRRRRRRRRRRRRRRRRTTTT")
        ( (chr,pos,orientation) , (chr2,pos2,orientation2) ) = get_breakpoint_info(record)
        record.ALT[0] = vcfpy.BreakEnd(mate_chrom=chr,mate_pos = pos2, orientation = orientation, mate_orientation=orientation2,sequence="",within_main_assembly=True)
        record.INFO["SVTYPE"] = "BND"
        writer.write_record(record)
        record.CHROM = chr2
        record.POS = pos2
        record.ALT[0] = vcfpy.BreakEnd(mate_chrom=chr,mate_pos = pos, orientation = orientation2, mate_orientation=orientation,sequence="",within_main_assembly=True)
        writer.write_record(record)

    
#########################
## Write the filtered VCF
reader = vcfpy.Reader.from_path(args.i)
header = reader.header
#header.add_filter_line({"ID":"MAPPABILITY","Description":"Regions with low mappability close to the breakpoint."})
writer = vcfpy.Writer.from_path(args.o,reader.header)
n_filter_PoN = 0
n_filter_length=0
n_filter_support=0
n_filter_polyA=0
for record in reader:
    if len(record.FILTER)>0:
        if len(record.FILTER)>1 or record.FILTER[0]!="PASS": continue
    reliable=True # will be set to False if the call is uncertain. unreliable SVs which are not completely filtered out will be put in a different file. 
    print("-----------")
    print(record)
    ( (chr,pos,orientation) , (chr2,pos2,orientation2) ) = get_breakpoint_info(record)
    if (not chr in chromosomes) or (not chr2 in chromosomes2):
        print("Filtered out because of unknown chromosome")
        continue
    if not chr2 in chromosomes: reliable=False
    
   
    # Filter based on minimum number of supporting reads
    SR = record.calls[args.tumorindex].data["SR"]
    PR = record.calls[args.tumorindex].data["RP"]
    if (PR <args.minPR//2 and record.calls[args.tumorindex].data["ASRP"] <2) or SR<args.minSR//2:
        print("Filtered out because of insufficient number of supporting reads")
        continue
    if (PR < args.minPR and record.calls[args.tumorindex].data["ASRP"] <args.minPR) or SR<args.minSR:
        print("Unreliable because of insufficient number of supporting reads")
        reliable = False
    if args.puretumor==0 and record.calls[args.tumorindex].data["REF"]+record.calls[args.tumorindex].data["REFPAIR"]<2:
        print("Unreliable because of insufficient number of no reference reads")
        reliable = False
    

    # Filter based on Panel of Normal
    if reliable:
        one_breakend_filtered = pos_is_filtered[(chr,pos)] or pos_is_filtered[(chr2,pos2)]
        if one_breakend_filtered:
            print("SV filtered by old Panel of Normal")
            reliable=False
            continue

        # Filter based on minimum SV length
        filtered_length = (chr==chr2) and ( abs(pos-pos2) < args.minLen)
        if filtered_length:
            print("SV filtered due to short length")
            n_filter_length+=1
            reliable=False
            #continue
   

    # Filter based on minimum number of supporting reads
    if reliable and filter_min_SR_PR(samfile,chr,pos,chr2,pos2):
        print("Filtered out because there were not enough reads supporting the SV, when removing reads with low mapq.")
        n_filter_support+=1
        reliable=False
        #continue

    # Filter out SVs where many of the reads in the region have low MAPQ, since these regions are unreliable.
    if reliable and args.tumorindex==0 and (low_mapq_in_region(samfile,chr,pos-100,pos+100,30) or low_mapq_in_region(samfile,chr2,pos2-100,pos2+100,30)):
        print("Filtered out because many of the reads near the breakpoint had a low MAPQ.")
        #continue
        reliable=False

    # Filter out SVs where the supporting reads have low MAPQ
    #if low_mapq_SV(samfile,chr,pos,chr2,pos2,args.minPR) or low_mapq_SV(samfile,chr2,pos2,chr,pos,args.minPR):
    #    print("Filtered out because many reads supporting the SV had a low MAPQ.")
    #    continue

    # Filter out SVs in regions where there are reads whose mates map to many different regions.
    #stringent = not (pos_close_to_CNA(chr,pos) or pos_close_to_CNA(chr2,pos2))
    strigent=False
    #if many_mates_in_region(samfile,chr,pos,stringent) or many_mates_in_region(samfile,chr2,pos2,stringent):
    #    print("Filtered out because reads in the region around the SV had mates mapped to many different regions")
    #    continue


    # Filter out reads with PolyA insertions, since they probably come from retrotransposons
    if "sequence" in dir(record.ALT[0]) and ("AAAAAAAAA" in record.ALT[0].sequence or "TTTTTTTTT" in record.ALT[0].sequence):
        print("PolyA insertion: probably retrotransposon --> filter out")
        n_filter_polyA+=1
        continue 

    # Check if deletions and duplications are validated by read depth, in which case we accept them directly. 
    """
    if record.INFO["SVTYPE"] == "DEL" and global_coverage>=0:
        coverage_del = compute_coverage_region(samfile,chr,pos,pos2)
        if coverage_del < 0.75 * global_coverage:
            print("Deletion confirmed based on read depth")
            writer.write_record(record)
            continue
        else:
            print("Deletion not confirmed based on read depth") 
            # Consider as BND instead of deletion...
            record.ALT[0] = vcfpy.BreakEnd(mate_chrom=chr,mate_pos = pos2, orientation = "-", mate_orientation="+",sequence="",within_main_assembly=True)
    elif record.INFO["SVTYPE"] == "DUP" and global_coverage>=0:
        coverage_dup = compute_coverage_region(samfile,chr,pos,pos2)
        if coverage_dup > 1.25 * global_coverage:
            print("Duplication confirmed based on read depth")
            writer.write_record(record)
            continue
        else:
            print("Duplication not confirmed based on read depth") 
            # Consider as BND instead of duplication...
            record.ALT[0] = vcfpy.BreakEnd(mate_chrom=chr,mate_pos = pos2, orientation = orientation, mate_orientation=orientation2,sequence="",within_main_assembly=True)
    """

    #if region_contains_genes(chr,pos-10000,pos+10000) or region_contains_genes(chr2,pos2-10000,pos2+10000):
    #    print("Breakpoint close to gene")
        #writer.write_record(record)
        #continue
    if reliable and args.tumorindex==0 and args.filterSmallInsertions==1:
        # See if a second SV can, combined with the first one, lead to a small insertion, which would then be filtered out.
        filter_out_record = False
        for r in collect_SVs_close(chr,pos,window_size):
            print(r)
            ( (Bchr,Bpos,Borientation) , (Bchr2,Bpos2,Borientation2) ) = get_breakpoint_info(r)
            if Bchr2 ==chr2 and abs(pos2-Bpos2) < window_size:
                # WARNING: Can have reciprocal translocation with small duplication, which would then look like a small insertion... 
                # In principle, we could use the full read information with the BAM, to check... 
                if pos < Bpos:
                    orientationOutward = (orientation=="+")
                    BorientationOutward = (Borientation=="-")
                else:
                    orientationOutward = (orientation=="-")
                    BorientationOutward = (Borientation=="+")
                if pos2<Bpos2:
                    orientationOutward2 = (orientation2=="+")
                    BorientationOutward2 = (Borientation2=="-")
                else:
                    orientationOutward2 = (orientation2=="-")
                    BorientationOutward2 = (Borientation2=="+")
                
                if abs(pos-Bpos)<120 and abs(pos2-Bpos2)<120: 
                    if reads_go_through_insertion(samfile,chr,pos,chr2,pos2):
                        print("Small insertion, because some read pairs going through the whole insertion were detected")
                        filter_out_record=True
                    elif reads_bordered_by_two_breakpoints(samfile,chr,pos,Bpos) or reads_bordered_by_two_breakpoints(samfile,chr2,pos2,Bpos2):
                        print("Small insertion, because some reads have clipping at both breakends.")
                        filter_out_record=True
                elif reads_bordered_by_two_breakpoints(samfile,chr,pos,Bpos) or reads_bordered_by_two_breakpoints(samfile,chr2,pos2,Bpos2):
                    print("Small insertion, because some reads have clipping at both breakends.")
                    filter_out_record=True
                elif abs(pos-Bpos)<1000 and abs(pos2-Bpos2)<1000 and args.keepCloseSV:
                    print("Close SVs") 
                elif (not orientationOutward) and (not BorientationOutward) and (not orientationOutward2) and (not BorientationOutward2):
                    if chr ==chr2:
                        print("Inversion") # TODO: inverted duplication ??
                    else:
                        print("Reciprocal translocation")
                elif orientationOutward and BorientationOutward and orientationOutward2 and BorientationOutward2 and ((abs(pos-Bpos)<=25 and abs(pos2-Bpos2)>130) or (abs(pos2-Bpos2)<=25 and abs(pos-Bpos)>130) or (not "SR" in r.calls[args.tumorindex].data)):
                    # Insertion with TSD. The duplicated part must be small, but the insertion large (otherwise we would have found a read going through the insertion.)
                    print("Small insertion with TSD ")
                    filter_out_record = True
                    continue
                elif ( orientationOutward and BorientationOutward and (not orientationOutward2) and not (BorientationOutward2) and ((not "SR" in r.calls[args.tumorindex].data) or (abs(pos2-Bpos2)<=25 and abs(pos-Bpos)>130)) ) \
                    or ( (not orientationOutward) and (not BorientationOutward) and orientationOutward2 and BorientationOutward2 and ((not "SR" in r.calls[args.tumorindex].data) or (abs(pos-Bpos)<=25 and abs(pos2-Bpos2)>130))):
                    # Insertion from the first side into the second side, with a deletion at the insertion site. The deletion must be <=25bp, and the insertion larger than 130bp, otherwise we would find reads going through the insertion
                    print("Small insertion, with a small deletion at the inserted site -")
                    filter_out_record = True
                    continue
                elif abs(pos-Bpos)<=5: # The orientation might be wrong...
                    if (not orientationOutward) and (not BorientationOutward) and orientationOutward2 and BorientationOutward2 and (( abs(pos2-Bpos2)>130) or (not "SR" in r.calls[args.tumorindex].data)):
                        # Insertion with TSD. The duplicated part must be small, but the insertion large (otherwise we would have found a read going through the insertion.)
                        print("Small insertion with TSD ")
                        filter_out_record = True
                        continue
                    elif (orientationOutward) and (BorientationOutward) and orientationOutward2 and BorientationOutward2 and ((not "SR" in r.calls[args.tumorindex].data) or  abs(pos2-Bpos2)>130):
                        # Insertion from the first side into the second side, with a deletion at the insertion site. The deletion must be <=25bp, and the insertion larger than 130bp, otherwise we would find reads going through the insertion
                        print("Small insertion, with a small deletion at the inserted site -")
                        filter_out_record = True
                        continue
                elif abs(pos2-Bpos2)<=5: # orientation might be wrong.
                    if orientationOutward and BorientationOutward and (not orientationOutward2) and (not BorientationOutward2) and ( abs(pos-Bpos)>130 or (not "SR" in r.calls[args.tumorindex].data)):
                        # Insertion with TSD. The duplicated part must be small, but the insertion large (otherwise we would have found a read going through the insertion.)
                        print("Small insertion with TSD ")
                        filter_out_record = True
                        continue
                    elif orientationOutward and BorientationOutward and orientationOutward2 and BorientationOutward2 and ((not "SR" in r.calls[args.tumorindex].data)and abs(pos-Bpos)>130):
                        # Insertion from the first side into the second side, with a deletion at the insertion site. The deletion must be <=25bp, and the insertion larger than 130bp, otherwise we would find reads going through the insertion
                        print("Small insertion, with a small deletion at the inserted site -")
                        filter_out_record = True
                        continue
                else:
                    print("Rearrangement is not clear -> keep.")
            print("-")

        if filter_out_record and (args.filterSmallInsertions ==1): 
            print("Should have been filtered out")
            reliable=False
            #continue
        
        if args.tumorindex==0:
            if SV_explained_by_alternative_alignments(samfile,chr,pos,chr2,pos2) or SV_explained_by_alternative_alignments(samfile,chr2,pos2,chr,pos):
                print("SV explained by alternative alignment.")
                #if stringent: continue
                reliable=False
                #continue
   
    if reliable:
        print("Keep SV")
        write_record(writer,record)

    else:
        print("Filtered out")
        continue

print((n_filter_PoN,n_filter_support,n_filter_length,n_filter_polyA))

if args.log is not None:
    log_file.close()